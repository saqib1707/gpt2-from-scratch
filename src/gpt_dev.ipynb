{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT - Generatively Pretrained Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wget\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18;43m__file__\u001b[39;49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.path.dirname(__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget('https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of characters in the dataset:  1115394\n"
     ]
    }
   ],
   "source": [
    "print('total number of characters in the dataset: ', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "print(''.join(vocab))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {s:i for i, s in enumerate(vocab)}\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "encode = lambda seq: [stoi[s] for s in seq]\n",
    "decode = lambda seq: ''.join([itos[i] for i in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "print(encode('hii there'))\n",
    "print(decode(encode('hii there')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "torch.Size([1003854]) torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "n = int(data.shape[0] * 0.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
      "when context is [18], then target is 47\n",
      "when context is [18, 47], then target is 56\n",
      "when context is [18, 47, 56], then target is 57\n",
      "when context is [18, 47, 56, 57], then target is 58\n",
      "when context is [18, 47, 56, 57, 58], then target is 1\n",
      "when context is [18, 47, 56, 57, 58, 1], then target is 15\n",
      "when context is [18, 47, 56, 57, 58, 1, 15], then target is 47\n",
      "when context is [18, 47, 56, 57, 58, 1, 15, 47], then target is 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8    # context length\n",
    "sample = train_data[:block_size+1]\n",
    "print(sample)\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = sample[0:t+1]\n",
    "    target = sample[t+1]\n",
    "    print(f'when context is {context.tolist()}, then target is {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 5\n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 8]) torch.Size([5, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54],\n",
      "        [57, 43, 60, 43, 52,  1, 63, 43]])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39],\n",
      "        [43, 60, 43, 52,  1, 63, 43, 39]])\n",
      "when context is tensor([24]), target is 43\n",
      "when context is tensor([24, 43]), target is 58\n",
      "when context is tensor([24, 43, 58]), target is 5\n",
      "when context is tensor([24, 43, 58,  5]), target is 57\n",
      "when context is tensor([24, 43, 58,  5, 57]), target is 1\n",
      "when context is tensor([24, 43, 58,  5, 57,  1]), target is 46\n",
      "when context is tensor([24, 43, 58,  5, 57,  1, 46]), target is 43\n",
      "when context is tensor([24, 43, 58,  5, 57,  1, 46, 43]), target is 39\n",
      "when context is tensor([44]), target is 53\n",
      "when context is tensor([44, 53]), target is 56\n",
      "when context is tensor([44, 53, 56]), target is 1\n",
      "when context is tensor([44, 53, 56,  1]), target is 58\n",
      "when context is tensor([44, 53, 56,  1, 58]), target is 46\n",
      "when context is tensor([44, 53, 56,  1, 58, 46]), target is 39\n",
      "when context is tensor([44, 53, 56,  1, 58, 46, 39]), target is 58\n",
      "when context is tensor([44, 53, 56,  1, 58, 46, 39, 58]), target is 1\n",
      "when context is tensor([52]), target is 58\n",
      "when context is tensor([52, 58]), target is 1\n",
      "when context is tensor([52, 58,  1]), target is 58\n",
      "when context is tensor([52, 58,  1, 58]), target is 46\n",
      "when context is tensor([52, 58,  1, 58, 46]), target is 39\n",
      "when context is tensor([52, 58,  1, 58, 46, 39]), target is 58\n",
      "when context is tensor([52, 58,  1, 58, 46, 39, 58]), target is 1\n",
      "when context is tensor([52, 58,  1, 58, 46, 39, 58,  1]), target is 46\n",
      "when context is tensor([25]), target is 17\n",
      "when context is tensor([25, 17]), target is 27\n",
      "when context is tensor([25, 17, 27]), target is 10\n",
      "when context is tensor([25, 17, 27, 10]), target is 0\n",
      "when context is tensor([25, 17, 27, 10,  0]), target is 21\n",
      "when context is tensor([25, 17, 27, 10,  0, 21]), target is 1\n",
      "when context is tensor([25, 17, 27, 10,  0, 21,  1]), target is 54\n",
      "when context is tensor([25, 17, 27, 10,  0, 21,  1, 54]), target is 39\n",
      "when context is tensor([57]), target is 43\n",
      "when context is tensor([57, 43]), target is 60\n",
      "when context is tensor([57, 43, 60]), target is 43\n",
      "when context is tensor([57, 43, 60, 43]), target is 52\n",
      "when context is tensor([57, 43, 60, 43, 52]), target is 1\n",
      "when context is tensor([57, 43, 60, 43, 52,  1]), target is 63\n",
      "when context is tensor([57, 43, 60, 43, 52,  1, 63]), target is 43\n",
      "when context is tensor([57, 43, 60, 43, 52,  1, 63, 43]), target is 39\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, batch_size=5):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    start_idxs = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in start_idxs])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in start_idxs])\n",
    "    return x, y\n",
    "\n",
    "x_batch, y_batch = get_batch('train')\n",
    "print(x_batch.shape, y_batch.shape)\n",
    "print(x_batch)\n",
    "print(y_batch)\n",
    "\n",
    "for x, y in zip(x_batch, y_batch):\n",
    "    for t in range(block_size):\n",
    "        context = x[0:t+1]\n",
    "        target = y[t]\n",
    "        print(f'when context is {context}, target is {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([58, 58, 58, 54, 63])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch.max(dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.1437, -1.5925, -2.6091,  ..., -1.7312, -1.3890, -2.8179],\n",
       "         [-2.2564, -2.0667, -4.2918,  ..., -2.2255, -2.8801, -2.9464],\n",
       "         [-3.1895, -3.1137, -1.0885,  ..., -4.1962, -2.3154, -2.0492],\n",
       "         ...,\n",
       "         [-0.4255, -1.6778, -2.0062,  ..., -1.8107, -2.0980, -2.6525],\n",
       "         [-0.1581, -1.7713, -0.5359,  ..., -3.2353, -1.7776, -1.6778],\n",
       "         [-2.2564, -2.0667, -4.2918,  ..., -2.2255, -2.8801, -2.9464]],\n",
       "\n",
       "        [[-2.0594, -2.6820, -2.6960,  ..., -3.4306, -5.4442, -3.8636],\n",
       "         [-2.2182, -1.6790, -3.2129,  ..., -1.8255, -2.6331, -2.5374],\n",
       "         [-2.1677, -4.7835, -1.5791,  ...,  0.0000, -1.5278, -0.6059],\n",
       "         ...,\n",
       "         [-4.3497, -5.1640, -4.3130,  ..., -2.8290, -4.5995, -3.6540],\n",
       "         [-2.5800, -2.1846, -3.3516,  ..., -0.1258, -2.6425, -2.8347],\n",
       "         [-2.1156, -2.6152, -2.6351,  ..., -2.8638, -1.8345, -3.4606]],\n",
       "\n",
       "        [[-2.5800, -2.1846, -3.3516,  ..., -0.1258, -2.6425, -2.8347],\n",
       "         [-0.1581, -1.7713, -0.5359,  ..., -3.2353, -1.7776, -1.6778],\n",
       "         [-4.3497, -5.1640, -4.3130,  ..., -2.8290, -4.5995, -3.6540],\n",
       "         ...,\n",
       "         [-3.2764, -3.8477, -1.6539,  ..., -2.7629, -1.2709, -2.9300],\n",
       "         [-4.3497, -5.1640, -4.3130,  ..., -2.8290, -4.5995, -3.6540],\n",
       "         [-2.4826, -2.0657, -2.9507,  ..., -2.0076, -0.9795, -2.6341]],\n",
       "\n",
       "        [[ 0.0000, -1.2618, -2.2542,  ..., -2.6057, -2.9862, -3.5668],\n",
       "         [-0.1581, -1.7713, -0.5359,  ..., -3.2353, -1.7776, -1.6778],\n",
       "         [-1.4376, -3.1314, -2.3330,  ..., -2.5360, -1.3410, -2.9352],\n",
       "         ...,\n",
       "         [-1.2508, -2.5507,  0.0000,  ..., -2.6875, -1.8364, -1.4680],\n",
       "         [-1.1482, -0.9199, -1.3244,  ..., -2.1190, -2.1859, -2.1243],\n",
       "         [-1.9486, -1.0158, -3.2429,  ...,  0.0000, -1.7331, -2.1441]],\n",
       "\n",
       "        [[-4.3497, -5.1640, -4.3130,  ..., -2.8290, -4.5995, -3.6540],\n",
       "         [-2.4826, -2.0657, -2.9507,  ..., -2.0076, -0.9795, -2.6341],\n",
       "         [-1.6677, -1.1944, -0.7722,  ..., -4.7979, -2.3141, -1.7733],\n",
       "         ...,\n",
       "         [-3.1895, -3.1137, -1.0885,  ..., -4.1962, -2.3154, -2.0492],\n",
       "         [-3.1895, -3.1137, -1.0885,  ..., -4.1962, -2.3154, -2.0492],\n",
       "         [-0.4255, -1.6778, -2.0062,  ..., -1.8107, -2.0980, -2.6525]]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out - out.max(dim=2)[0].unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 65])\n",
      "tensor(4.5264, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)    # [B,T,C]\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # counts = (logits - logits.max(dim=2)[0].unsqueeze(-1)).exp()\n",
    "            # probs = counts / counts.sum(dim=2, keepdims=True)  # [B,T,C]\n",
    "            # loss2 = -probs[:, :, targets].log().mean()\n",
    "\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)  # [B,T,C]\n",
    "            logits = logits[:, -1, :]    # [B,C]\n",
    "            probs = F.softmax(logits, dim=-1)    # [B,C]\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)    # [B,1]\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "            # print(idx.shape)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "out, loss = model(x_batch, y_batch)\n",
    "print(out.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bb3r,Z3vrBwgJQW XeT &?aoV'.WgZWh-H,A'wh?q!eF&:dh:dTFPuWcJloiAbUvwoIuvCEgvl3AOGkKmb&WDsprzJFPFqQWSe-YP?zQNrzXr-O3NNG&bgWhq?oZDmB':hWQq-'ECt$!S.!u;C$gyYHQDwiBuk\n",
      "DsEb$KofVFtTbIxQYRwkwiHznhoqsI?iPH:PFNUerDh?SyfAbB-w3AsVwDDh.mHWFUsoJqB$?MEQNZ$Vuis-mBT3pxBlEoAbNBCCQKjr$Nm:b\n",
      "?ndag;Dsk,-xk$?zay,oEO,DuFPFoV3H:SAgCDtcq\n",
      "-ux\n",
      "P'gWI,MasQlPM'gK$gyCxwF?Nii.mr;V,R E;e&,dFgxqDtWTEJOSKnztae'WwTFeTmhkmc,bBxUJ!GbKVM.gZCN3&f-PLiB-U&daEJC lHYzUdfyfd.MaAbtkp.lCm?P$cttUwDALJRkARA'cnKoZv.rUDc.!c.oagW,R,lLtNBw-ux!,yZnYeARD$nXIWCu,ou,dD.o,-RnnZLEHgcM\n",
      "MebyYByf&WLRG 'H:mIv?o!pWHDHe3ARjYa,Yzt$dxLHqI-liXc3Xy-t$dTgt$sj,gg'Xylof ChfX'yYeFgJ:XAoA.EZ$gWEknU$CbvSEdI  ozY?akLu,oKgm&-xOuBIOPYTjQCk:MbZvlou 'Uv3'M\n",
      "xQDhtHvBQQx?zH$-u-t&LB'yJ!&WSKDHIx!tprwC Ou-u 'Hq;P!\n",
      "cxOPmbyc!mjzRo'YE?zsEBI,uPQDnZAF IRs \n",
      "X;nYM,gH:PLpMXEZ$:\n",
      "PzG &QAefPcldcSJCHDJK'ySh?fZ$V!pV!pB-loQQKg\n",
      ",zvXJeePKGFb;Af MQAMGGGTSevXAoYTfDqShkdz,OSb3-TBu,sfOkcWbOQAtUsAzcLsPwielAw;dzJVGmcTtMIV!ag&$rB'.i-xrUGMHYTOHoPD\n",
      "kQqjdySSJVHtLrHDjQhHOiG.;Whp&DwFU$NRYT!mlJeupeV3Z\n"
     ]
    }
   ],
   "source": [
    "inp = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(model.generate(idx=inp, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:45<00:00, 1102.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.425154447555542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for itr in tqdm(range(50000)):\n",
    "    x_batch, y_batch = get_batch('train', batch_size)\n",
    "\n",
    "    out, loss = model(x_batch, y_batch)\n",
    "    # for param in model.parameters():\n",
    "    #     param.grad = None\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S:\n",
      "BRWishs lay\n",
      "KIOrlshanlindate irand pl m d? sccrveofod le\n",
      "METht ich the st arreral ivernonyom fe t if teairt pimom. ot buingo n fouly cqud s thio, acrg; s.\n",
      "Tr fowa sshekn Paned: sont d k?\n",
      "Br torero,-cee,\n",
      "NRARIUCo thastwhe t on cend: iooshugf shen t beand;\n",
      "Oniomye t deive.\n",
      "Fougin.\n",
      "\n",
      "My ton, ES:\n",
      "ANONonflaf arnodesthoo hoo!\n",
      "Al by\n",
      "Tot perne the emad fore E toushayak s od lld hathy te t halousht, y soo w, n'd tho by on henss, canama e se rone; is tothothaus pt s aro'\n",
      "I ty'th.\n",
      "HANowid tis\n",
      "Dolir maintha t\n",
      "S:\n",
      "YO: keazeaken oun: nt mun.\n",
      "Winoun?\n",
      "Ofeniowesthounse hilort ckirighirdovensomar\n",
      "Towndithid puitariowo Ithind:\n",
      "Ha blit e he 'd d\n",
      "mep,\n",
      "Aloilo,\n",
      "\n",
      "G gr ABUng, indlin.\n",
      "PUCENGLI he KIf thenave waryokiey tl noun w?\n",
      "\n",
      "\n",
      "Whivenat herefe man.\n",
      "T:\n",
      "Bin her.\n",
      "ARGLLINags, eviva te cos omoronoprmpin hend br h br re.\n",
      "\n",
      "\n",
      "\n",
      "D pisor hounde thablllfragen he fuy, a l. lopry thothenty blaf b the\n",
      "CE:\n",
      "coty nes.\n",
      "QUFIZARGle s?\n",
      "AUCHil ang habucokssovommor.\n",
      "\n",
      "Ifo g d belo ngol t:\n",
      "\n",
      "RI:\n",
      "La s blipin me.\n",
      "Theer moroneancorst d w\n"
     ]
    }
   ],
   "source": [
    "inp = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(model.generate(idx=inp, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy example\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]    # [t+1,C]\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0)\n",
    "# xbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[4., 5.],\n",
      "        [2., 7.],\n",
      "        [1., 0.]])\n",
      "tensor([[4.0000, 5.0000],\n",
      "        [3.0000, 6.0000],\n",
      "        [2.3333, 4.0000]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones((3,3)))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2\n",
    "wei = torch.tril(torch.ones((T, T)))\n",
    "wei = wei / torch.sum(wei, dim=1, keepdims=True)\n",
    "print(wei)\n",
    "xbow2 = wei @ x     # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "xbow2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(-5.1456e-07)\n"
     ]
    }
   ],
   "source": [
    "print(torch.allclose(xbow, xbow2))\n",
    "print((xbow - xbow2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3\n",
    "tril = torch.tril(torch.ones((T, T)))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "# print(wei)\n",
    "xbow3 = wei @ x\n",
    "print(torch.allclose(xbow, xbow3))\n",
    "xbow3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = nn.Embedding(65, 32)\n",
    "tmp1 = torch.arange(32)\n",
    "tmp(tmp1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65])\n",
      "tensor(4.5233, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_embd = 32\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        tok_emb = self.token_embedding_table(idx)    # (B,T,C=n_embd)\n",
    "        pos_emb = self.pos_embedding_table(torch.arange(block_size))    # (T,C=n_embd)\n",
    "        x = tok_emb + pos_emb    # (B,T,C)\n",
    "        logits = self.lm_head(x)    # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # counts = (logits - logits.max(dim=2)[0].unsqueeze(-1)).exp()\n",
    "            # probs = counts / counts.sum(dim=2, keepdims=True)  # [B,T,C]\n",
    "            # loss2 = -probs[:, :, targets].log().mean()\n",
    "\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)  # [B,T,C]\n",
    "            logits = logits[:, -1, :]    # [B,C]\n",
    "            probs = F.softmax(logits, dim=-1)    # [B,C]\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)    # [B,1]\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "            # print(idx.shape)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "out, loss = model(x_batch, y_batch)\n",
    "print(out.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each token will emit two vectors\n",
    "query vector for each token - what I am looking for\n",
    "key vector for each token - what do I contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention (is a communication mechanism)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# single-head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "val = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)    # (B,T,16)\n",
    "q = query(x)    # (B,T,16)\n",
    "wei = q @ k.transpose(-2,-1)    # (B,T,16) @ (B,16,T) --> (B,T,T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))  # (T,T)\n",
    "# wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # (B,T,T)\n",
    "wei = F.softmax(wei, dim=-1)  # (B,T,T)\n",
    "\n",
    "v = val(x)    # (B,T,16)\n",
    "out = wei @ v  # (B,T,T) @ (B,T,16) --> (B,T,16)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't use starred expression here (1713185751.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[119], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    *[i * 2 for i in range(10)]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't use starred expression here\n"
     ]
    }
   ],
   "source": [
    "[i * 2 for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4365, 0.5635, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0202, 0.2026, 0.7771, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5387, 0.1885, 0.0444, 0.2285, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2233, 0.0607, 0.0150, 0.5439, 0.1571, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.1686, 0.0208, 0.0668, 0.4425, 0.1012, 0.0000, 0.0000],\n",
       "        [0.1184, 0.1106, 0.0032, 0.5313, 0.0572, 0.0592, 0.1199, 0.0000],\n",
       "        [0.1300, 0.0152, 0.0033, 0.0166, 0.1057, 0.0346, 0.4233, 0.2713]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.randn(B,T,head_size)  # (B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)  # (B,T,head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2,-1) / (head_size**0.5)  # (B,T,T)\n",
    "wei.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9252)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1416)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0104)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
