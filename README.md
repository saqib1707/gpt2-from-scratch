# GPT-2 Implementation in PyTorch

This project reproduces the GPT-2 model in pytorch and trains it from scratch on the FineWeb-Edu dataset - a high-quality subset of FineWeb dataset tailored for educational content. The goal is to offer a simplified, easy-to-understand PyTorch implementation. Note that this code is intended primarily for educational purposes and is not optimized for speed or production deployment.

### Key Features
- **Simplified PyTorch Implementation:** Designed to be accessible and well-commented for ease of understanding.
- **Customizable Training:** Hyperparameters are configurable via the command line and can be easily modified.
- **Multi-GPU Training Support:** Training can be performed using multiple GPUs using PyTorch Distributed Data Parallel (DDP).


## Repository Structure
- `src/train.py`: Script to train the GPT-2 model with customizable configurations.
- `src/model.py`: Contains the GPT-2 model implementation, including embedding layers, transformer blocks, and output layers.
- `src/dataloader.py`:  Handles data loading and batching for the model during training.
- `src/prepare_dataset.py`: Downloads and preprocesses the FineWebEdu dataset. Run this script before starting the training process.
- `requirements.txt`: Python dependencies required to run the project.


## Getting Started

### Prerequisites
Ensure you have the following dependencies installed:

- numpy
- pytorch
- tiktoken
- transformers (from huggingface)

You can install all dependencies with:
```bash
pip install -r requirements.txt
```

## Dataset

The GPT-2 model was originally trained on the WebText dataset (not publicly released). For this project, we use the FineWebEdu-10B dataset—a specialized educational subset of the FineWeb dataset. It contains approximately 10 billion tokens focused on high-quality educational content.

To download and prepare the dataset:
```bash
python prepare_dataset.py
```

### Running the Training Script
You can start training the GPT-2 model using the following commands:

You can experiment with different training and model config hyperparameters by setting them through the command line. 

- Single-GPU Training:
```bash
python train.py --num_epochs=5
```

- Multi-GPU Training (uses Pytorch DDP):
```bash
torchrun --standalone --nproc_per_node=4 train.py    # adjust number of GPUs as per availability
```

For more details on the training process and customizing hyperparameters, refer to the `src/train.py` script.

Training was performed from scratch using multiple GPUs with PyTorch's DDP framework.


### Model Architecture
The GPT-2 model consists of the following components:

- **Token Embedding Layer:** Encodes input tokens to dense vectors.
- **Positional Embedding Layer:** Adds positional information to the token embeddings.
- **Transformer Blocks:** Each block includes layer normalization, multi-headed self-attention, and an MLP with residual connections.
- **Output Head:** Predicts the next token in the sequence based on the preceding context.

The model is trained to predict the next token in a sequence, enabling coherent text generation.


### Results

The GPT-2 model was trained for roughly 50,000 steps (~3 epochs) using 4 NVIDIA A100 GPUs. Training took approximately 21 hours.

![Training loss and eval](./assets/loss_eval.png)

Here are some samples of text generated by the trained model: 

*To be updated...*


<!-- ## Potential Future Work

1.  -->


## References:
- [Language Models are Unsupervised Multitask Learners (GPT-2 Paper)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [GPT-3 Paper: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
- [FineWebEdu-10B Dataset](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
- [Attention is all you need](https://arxiv.org/abs/1706.03762)
- [HellaSwag: Can a Machine Really Finish Your Sentence?](https://arxiv.org/abs/1905.07830)
- Andrej Karpathy's Video Tutorial on GPT


## Acknowledgments
This implementation is inspired by Andrej Karpathy’s tutorial and his approach to making complex AI concepts more accessible.